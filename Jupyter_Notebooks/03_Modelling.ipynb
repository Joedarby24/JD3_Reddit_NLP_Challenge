{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows=100\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/clean_eda_model_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and TTS\n",
    "X = df['post_title']\n",
    "y = df['subred_binom']\n",
    "\n",
    "# Train, Test, Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   random_state=42)\n",
    "\n",
    "# Set up a piple line for our models\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember our baseline rate is 56.51% for the model (1 = The_Donald)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joeda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score = 0.7870370370370371\n",
      "\n",
      "Training score = 0.9915123456790124'\n",
      "\n",
      "Test score = 0.8106235565819861\n",
      "\n",
      "Log Model overfit by -0.18088878909702621\n"
     ]
    }
   ],
   "source": [
    "# Gauging an un-tuned Logarithmic Regression model\n",
    "print(f'cross_val_score = {cross_val_score(pipe, X_train, y_train, cv=3).mean()}\\n')\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "print(f'''Training score = {pipe.score(X_train, y_train)}'\n",
    "\n",
    "Test score = {pipe.score(X_test, y_test)}\n",
    "\n",
    "Log Model overfit by {(pipe.score(X_test, y_test) - pipe.score(X_train, y_train))}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A un-tuned Logistic Regression model is overfit by 0.1819 (train (0.9915123456790124) - test (0.8106235565819861))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Gridsearch to a LogisticRegression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7839506172839507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 1729,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': None,\n",
       " 'lr__max_iter': 500,\n",
       " 'lr__solver': 'sag'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define features and TTS\n",
    "X = df['post_title']\n",
    "y = df['subred_binom']\n",
    "\n",
    "# Train, Test, Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   random_state=42)\n",
    "\n",
    "# Set up a piple line for our models\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(random_state= 42))\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__stop_words': [None],\n",
    "    'cvec__max_features': [1729],\n",
    "    'cvec__min_df': [1, 2],\n",
    "    'cvec__max_df': [.9, .8, .95, .99],\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'lr__solver': ['sag'],\n",
    "    'lr__max_iter': [500]\n",
    "                   }\n",
    "\n",
    "gscv_mod = GridSearchCV(pipe, param_grid = pipe_params, cv=3)\n",
    "gscv_mod.fit(X_train, y_train)\n",
    "print(gscv_mod.best_score_)\n",
    "gscv_mod.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score = 0.970679012345679\n",
      "\n",
      "test score = 0.8175519630484989 \n",
      "\n",
      "Log Model overfit by -0.15312704929718013\n"
     ]
    }
   ],
   "source": [
    "print(f'''training score = {gscv_mod.score(X_train, y_train)}\n",
    "\n",
    "test score = {gscv_mod.score(X_test, y_test)} \n",
    "\n",
    "Log Model overfit by {(gscv_mod.score(X_test, y_test) - gscv_mod.score(X_train, y_train))}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through Pipline & GridSearchCV tuning, the best training score that I was able to obtain was 0.78395. I found the best parameters for \"cvec\" and \"lr\" to be:\n",
    "- 'cvec__max_df': 0.9\n",
    "- 'cvec__max_features': 1729\n",
    "- 'cvec__min_df': 1\n",
    "- 'cvec__ngram_range': (1, 1\n",
    "- 'cvec__stop_words': None\n",
    "- 'lr__max_iter': 500\n",
    "- 'lr__solver': 'sag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gscv_mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[148,  52],\n",
       "       [ 27, 206]], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to calculate and return our classification metrics for confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_metrics(tp, tn, fn, fp):\n",
    "    accuracy = (tp + tn) / (tp + tn + fn +fp)\n",
    "    missclass = (fn + fp) / (tp + tn + fn + fp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    print(f'Accuracy = {accuracy}\\nMissclassification = {missclass}\\nSensitivity = {sensitivity}\\nSpecificity = {specificity}\\nPrecision = {precision}')\n",
    "    \n",
    "    return accuracy, missclass, sensitivity, specificity, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 148\n",
      "False Positives: 52\n",
      "False Negatives: 27\n",
      "True Positives: 206\n"
     ]
    }
   ],
   "source": [
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8175519630484989\n",
      "Missclassification = 0.18244803695150116\n",
      "Sensitivity = 0.8841201716738197\n",
      "Specificity = 0.74\n",
      "Precision = 0.7984496124031008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8175519630484989,\n",
       " 0.18244803695150116,\n",
       " 0.8841201716738197,\n",
       " 0.74,\n",
       " 0.7984496124031008)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(tp, tn, fn, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7878086419753086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.25,\n",
       " 'cvec__max_features': 1729,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define features and TTS\n",
    "X = df['post_title']\n",
    "y = df['subred_binom']\n",
    "\n",
    "# Train, Test, Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y) \n",
    "\n",
    "# Set up a piple line for our models\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_features': [1729],\n",
    "    'cvec__min_df': [1],\n",
    "    'cvec__max_df': [.25],\n",
    "    'cvec__ngram_range': [(1,1)]\n",
    "                   }\n",
    "\n",
    "gscv_mod = GridSearchCV(pipe, param_grid = pipe_params, cv=3)\n",
    "gscv_mod.fit(X_train, y_train)\n",
    "print(gscv_mod.best_score_)\n",
    "gscv_mod.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9243827160493827"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_mod.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.792147806004619"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_mod.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score = 0.9243827160493827\n",
      "\n",
      "test score = 0.792147806004619 \n",
      "\n",
      "Log Model overfit by -0.13223491004476373\n"
     ]
    }
   ],
   "source": [
    "print(f'''training score = {gscv_mod.score(X_train, y_train)}\n",
    "\n",
    "test score = {gscv_mod.score(X_test, y_test)} \n",
    "\n",
    "Log Model overfit by {(gscv_mod.score(X_test, y_test) - gscv_mod.score(X_train, y_train))}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gscv_mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[148,  40],\n",
       "       [ 50, 195]], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 148\n",
      "False Positives: 40\n",
      "False Negatives: 50\n",
      "True Positives: 195\n"
     ]
    }
   ],
   "source": [
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.792147806004619\n",
      "Missclassification = 0.20785219399538107\n",
      "Sensitivity = 0.7959183673469388\n",
      "Specificity = 0.7872340425531915\n",
      "Precision = 0.8297872340425532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.792147806004619,\n",
       " 0.20785219399538107,\n",
       " 0.7959183673469388,\n",
       " 0.7872340425531915,\n",
       " 0.8297872340425532)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(tp, tn, fn, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
